{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Week5&6_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "nteract": {
      "version": "0.21.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vm00328/vm00328_coursework_com2025/blob/master/Week5%266_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9GCGMtwyohS"
      },
      "source": [
        "# Convolutional Neural Networks (CNN)\n",
        "\n",
        "In the first part of this lab session we will explore how __convolutions__ work, basic __CNN architectures__ and __impact of hyperparameters__. <br>\n",
        "In the second part you will integrate neural network trained in part 1, to the drawing program to continuously predict drawn object.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRp_qWdxyohW"
      },
      "source": [
        "## Basic Imports\n",
        "\n",
        "For this lab session, we will be needing Keras, Numpy and Tensorflow. <br>\n",
        "We will build our CNN in Keras, but first we need to understand the underlying principles, for which we will use Tensorflow. <br>\n",
        "Numpy will be mainly used for dataset preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P4TN6uLyohX"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "from numpy.random import seed\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coF3bUaByohe"
      },
      "source": [
        "## Replicability\n",
        "While experimenting and researching, it is important that your results can be __replicated by other people__. <br>\n",
        "To ensure some level of replicability, we can __set the starting seed__ of both numpy and tensorflow __to known value__. <br>\n",
        "Therefore, when we initialise our network to random values, these states can be calculated and replicated just by knowing the seed.<br>\n",
        "Sadly, due to the lossy nature of GPU calculations, training itself cannot be perfectly replicated, but it is still good practice to set initial seeds to known values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68_hPQu8yohf"
      },
      "source": [
        "seed(101)\n",
        "tf.random.set_seed(101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE6znDdhyohj"
      },
      "source": [
        "## Low Level Code\n",
        "\n",
        "While building model, we will add whole convolution layer in a neat package `Conv2D()`. <br>\n",
        "But before we do so, it is good to understand the __underlaying mechanics__ and __code implementation__ of convolutions. <br>\n",
        "In the following example, we will define our image/data array `inputs`, and using `kernel` apply 2D convolution. <br>\n",
        "Full implemenation of convolution layer can be found <a src=\"https://github.com/keras-team/keras/blob/8ed57c168f171de7420e9a96f9e305b8236757df/keras/layers/convolutional.py#L161\"> here</a>. <br>\n",
        "\n",
        "\n",
        "A convolution input must have shape of `(BatchSize, width, height, inputChannels)` <br>\n",
        "A convolution filter must have shape of `(width, height, inputChannels, outputChannels)` <br>\n",
        "\n",
        "### Task\n",
        "1) Experiment with different strides, kernel, padding etc. <br>\n",
        "2) Why does first ouput element equal to 21.0?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMy6HP4Wyohi"
      },
      "source": [
        "# Convolutions\n",
        "\n",
        "Convolutional Neural Networks (CNNs) are designed to learn features directly from image pixels. They can classify patterns or objects with extreme variability. Currently, they form the core of various __computer vision systems__ such as Facebooks automated photo tagging, handwritten characters recognition, self-driving cars, marine mammal detection, and medical image analysis. In this lab, we will start by exploring a convolution function which forms the heart of CNNs.\n",
        "\n",
        "![Convolution](https://raw.githubusercontent.com/RetinalSW/COM3025/master/data/convolution_kernal.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDUa_aP1yohj",
        "outputId": "f9d79083-68f6-444d-f651-466c8c3cc74d"
      },
      "source": [
        "# We need keras.backend and tensorflow to create proper tensors directly\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "inputs = tf.constant([[[1.0],  [2.0],  [3.0],  [4.0],  [5.0]],\n",
        "                      [[6.0],  [7.0],  [8.0],  [9.0],  [10.0]],\n",
        "                      [[11.0], [12.0], [13.0], [14.0], [15.0]],\n",
        "                      [[16.0], [17.0], [18.0], [19.0], [20.0]],\n",
        "                      [[21.0], [22.0], [23.0], [24.0], [25.0]]\n",
        "                     ])\n",
        "\n",
        "\n",
        "kernel = tf.constant([[1.0,0.0,0.0],\n",
        "                      [0.0,1.0,0.0],\n",
        "                      [0.0,0.0,1.0]])\n",
        "\n",
        "\n",
        "\n",
        "inputs = K.reshape(inputs,(-1,5,5,1))\n",
        "print(\"Shape of an Input:\", inputs.get_shape)\n",
        "\n",
        "kernel =K.reshape(kernel,(3,3,1,1))\n",
        "print(\"Shape of a Kernel\", kernel.get_shape)\n",
        "\n",
        "strides=(1, 1)\n",
        "padding='valid'\n",
        "\n",
        "result = K.conv2d(inputs, kernel, strides=strides, padding=padding)\n",
        "print(\"Shape of result:\", result.get_shape)\n",
        "\n",
        "print(\"Result:\", K.eval(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of an Input: <bound method _EagerTensorBase.get_shape of <tf.Tensor: shape=(1, 5, 5, 1), dtype=float32, numpy=\n",
            "array([[[[ 1.],\n",
            "         [ 2.],\n",
            "         [ 3.],\n",
            "         [ 4.],\n",
            "         [ 5.]],\n",
            "\n",
            "        [[ 6.],\n",
            "         [ 7.],\n",
            "         [ 8.],\n",
            "         [ 9.],\n",
            "         [10.]],\n",
            "\n",
            "        [[11.],\n",
            "         [12.],\n",
            "         [13.],\n",
            "         [14.],\n",
            "         [15.]],\n",
            "\n",
            "        [[16.],\n",
            "         [17.],\n",
            "         [18.],\n",
            "         [19.],\n",
            "         [20.]],\n",
            "\n",
            "        [[21.],\n",
            "         [22.],\n",
            "         [23.],\n",
            "         [24.],\n",
            "         [25.]]]], dtype=float32)>>\n",
            "Shape of a Kernel <bound method _EagerTensorBase.get_shape of <tf.Tensor: shape=(3, 3, 1, 1), dtype=float32, numpy=\n",
            "array([[[[1.]],\n",
            "\n",
            "        [[0.]],\n",
            "\n",
            "        [[0.]]],\n",
            "\n",
            "\n",
            "       [[[0.]],\n",
            "\n",
            "        [[1.]],\n",
            "\n",
            "        [[0.]]],\n",
            "\n",
            "\n",
            "       [[[0.]],\n",
            "\n",
            "        [[0.]],\n",
            "\n",
            "        [[1.]]]], dtype=float32)>>\n",
            "Shape of result: <bound method _EagerTensorBase.get_shape of <tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=\n",
            "array([[[[21.],\n",
            "         [24.],\n",
            "         [27.]],\n",
            "\n",
            "        [[36.],\n",
            "         [39.],\n",
            "         [42.]],\n",
            "\n",
            "        [[51.],\n",
            "         [54.],\n",
            "         [57.]]]], dtype=float32)>>\n",
            "Result: [[[[21.]\n",
            "   [24.]\n",
            "   [27.]]\n",
            "\n",
            "  [[36.]\n",
            "   [39.]\n",
            "   [42.]]\n",
            "\n",
            "  [[51.]\n",
            "   [54.]\n",
            "   [57.]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjIURbvsyohm"
      },
      "source": [
        "# Dropout\n",
        "Dropout consists of __randomly__ setting a fraction `rate` of input units to 0 at each update __during training time__\n",
        "which helps to __prevent overfitting__. <br>\n",
        "To balance the overall signal strength, we increase the non-zero outputs accordingly. \n",
        "\n",
        "\n",
        "## Arguments\n",
        "`rate`: float between 0 and 1. Fraction of the input units to drop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pPZrNnNyohm",
        "outputId": "324c6cff-3dc6-47c6-da0b-1ec8d94355a7"
      },
      "source": [
        "inputs = tf.constant([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
        "rate = 0.8\n",
        "\n",
        "result = K.dropout(inputs, rate, None)\n",
        "print(\"Result:\", K.eval(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Result: [0. 0. 0. 0. 5. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12ATZEXIyoho"
      },
      "source": [
        "# Pooling\n",
        "\n",
        "## MaxPooling\n",
        "\n",
        "Another important concept of CNNs is max-pooling, which is a form of\n",
        "non-linear down-sampling. Max-pooling partitions the input image into a\n",
        "set of non-overlapping rectangles and, for each such sub-region, outputs\n",
        "the maximum value.\n",
        "Max-pooling is useful in vision for two reasons:\n",
        "- By eliminating non-maximal values, it reduces computation for upper layers.\n",
        "- It provides a form of translation invariance.\n",
        "\n",
        "\n",
        "## AveragePooling\n",
        "Alternative to MaxPooling is Average pooling, where you take sum of all elements in pool and divide by number of elements. \n",
        "\n",
        "Experiment with different strides, kernel, padding etc. <br> \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "520X_-ziyoho",
        "outputId": "d71efa2d-18dd-4183-fe43-125a9f480895"
      },
      "source": [
        "inputs = tf.constant([[[1.0],  [2.0],  [3.0],  [4.0],  [5.0]],\n",
        "                      [[6.0],  [7.0],  [8.0],  [9.0],  [10.0]],\n",
        "                      [[11.0], [12.0], [13.0], [14.0], [15.0]],\n",
        "                      [[16.0], [17.0], [18.0], [19.0], [20.0]],\n",
        "                      [[21.0], [22.0], [23.0], [24.0], [25.0]]\n",
        "                     ])\n",
        "\n",
        "inputs = K.reshape(inputs,(-1,5,5,1))\n",
        "print(\"Shape of an Input:\", inputs.get_shape)\n",
        "\n",
        "pool_size = (2,2)\n",
        "strides=(1, 1)\n",
        "padding='valid'\n",
        "data_format=None\n",
        "pool_mode='max' # or use 'avg'\n",
        "\n",
        "result = K.pool2d(inputs, pool_size=pool_size, strides = strides,\n",
        "                          padding = padding, data_format = data_format,\n",
        "                          pool_mode=pool_mode)\n",
        "\n",
        "print(result.get_shape)\n",
        "print(\"Result:\", K.eval(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of an Input: <bound method _EagerTensorBase.get_shape of <tf.Tensor: shape=(1, 5, 5, 1), dtype=float32, numpy=\n",
            "array([[[[ 1.],\n",
            "         [ 2.],\n",
            "         [ 3.],\n",
            "         [ 4.],\n",
            "         [ 5.]],\n",
            "\n",
            "        [[ 6.],\n",
            "         [ 7.],\n",
            "         [ 8.],\n",
            "         [ 9.],\n",
            "         [10.]],\n",
            "\n",
            "        [[11.],\n",
            "         [12.],\n",
            "         [13.],\n",
            "         [14.],\n",
            "         [15.]],\n",
            "\n",
            "        [[16.],\n",
            "         [17.],\n",
            "         [18.],\n",
            "         [19.],\n",
            "         [20.]],\n",
            "\n",
            "        [[21.],\n",
            "         [22.],\n",
            "         [23.],\n",
            "         [24.],\n",
            "         [25.]]]], dtype=float32)>>\n",
            "<bound method _EagerTensorBase.get_shape of <tf.Tensor: shape=(1, 4, 4, 1), dtype=float32, numpy=\n",
            "array([[[[ 7.],\n",
            "         [ 8.],\n",
            "         [ 9.],\n",
            "         [10.]],\n",
            "\n",
            "        [[12.],\n",
            "         [13.],\n",
            "         [14.],\n",
            "         [15.]],\n",
            "\n",
            "        [[17.],\n",
            "         [18.],\n",
            "         [19.],\n",
            "         [20.]],\n",
            "\n",
            "        [[22.],\n",
            "         [23.],\n",
            "         [24.],\n",
            "         [25.]]]], dtype=float32)>>\n",
            "Result: [[[[ 7.]\n",
            "   [ 8.]\n",
            "   [ 9.]\n",
            "   [10.]]\n",
            "\n",
            "  [[12.]\n",
            "   [13.]\n",
            "   [14.]\n",
            "   [15.]]\n",
            "\n",
            "  [[17.]\n",
            "   [18.]\n",
            "   [19.]\n",
            "   [20.]]\n",
            "\n",
            "  [[22.]\n",
            "   [23.]\n",
            "   [24.]\n",
            "   [25.]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PZ0c4OppRLv"
      },
      "source": [
        "# Stride and Padding\n",
        "- You might have noticed the padding and stride, but do you know what the stride and padding exactly are?\n",
        "## Convolution animations\n",
        "\n",
        "_N.B.: Blue maps are inputs, and cyan maps are outputs._\n",
        "\n",
        "<table style=\"width:100%; table-layout:fixed;\">\n",
        "  <tr>\n",
        "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/arbitrary_padding_no_strides.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/same_padding_no_strides.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/full_padding_no_strides.gif\"></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>No padding, no strides</td>\n",
        "    <td>Arbitrary padding, no strides</td>\n",
        "    <td>Half padding, no strides</td>\n",
        "    <td>Full padding, no strides</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_strides.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides_odd.gif\"></td>\n",
        "    <td></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>No padding, strides</td>\n",
        "    <td>Padding, strides</td>\n",
        "    <td>Padding, strides (odd)</td>\n",
        "    <td></td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "## Transposed convolution animations\n",
        "\n",
        "_N.B.: Blue maps are inputs, and cyan maps are outputs._\n",
        "\n",
        "<table style=\"width:100%; table-layout:fixed;\">\n",
        "  <tr>\n",
        "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides_transposed.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/arbitrary_padding_no_strides_transposed.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/same_padding_no_strides_transposed.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/full_padding_no_strides_transposed.gif\"></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>No padding, no strides, transposed</td>\n",
        "    <td>Arbitrary padding, no strides, transposed</td>\n",
        "    <td>Half padding, no strides, transposed</td>\n",
        "    <td>Full padding, no strides, transposed</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_strides_transposed.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides_transposed.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides_odd_transposed.gif\"></td>\n",
        "    <td></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>No padding, strides, transposed</td>\n",
        "    <td>Padding, strides, transposed</td>\n",
        "    <td>Padding, strides, transposed (odd)</td>\n",
        "    <td></td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "## Dilated convolution animations\n",
        "\n",
        "_N.B.: Blue maps are inputs, and cyan maps are outputs._\n",
        "\n",
        "<table style=\"width:25%\"; table-layout:fixed;>\n",
        "  <tr>\n",
        "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/dilation.gif\"></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>No padding, no stride, dilation</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "### Reference: [https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mohrx8gns73e"
      },
      "source": [
        "## <font color='red'>Task 1</font>\n",
        "We have a input of (1x4x4x1)\n",
        "- Can you output make a Conv2D that give the output that is the same size as the input?\n",
        "- Can you make the output a half smaller than the original output? `(1x2x2x1)`\n",
        "- You can verify by printing out the result.shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf7qUPM0s3zk"
      },
      "source": [
        "inputs = tf.constant([[[1.0],  [2.0],  [3.0],  [4.0]],\n",
        "                      [[6.0],  [7.0],  [8.0],  [9.0]],\n",
        "                      [[16.0], [17.0], [18.0], [19.0]],\n",
        "                      [[21.0], [22.0], [23.0], [24.0]]\n",
        "                     ])\n",
        "\n",
        "inputs = K.reshape(inputs,(-1,4,4,1))\n",
        "print(\"Shape of an Input:\", inputs.shape)\n",
        "# Your code here (make the same size)\n",
        "\n",
        "# Make sure your output's shape is the same as the input\n",
        "\n",
        "# Your code here (make a half smaller)\n",
        "\n",
        "# Make sure your output's shape is the same as the input\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjKP2Gouyohq"
      },
      "source": [
        "# Flatten\n",
        "\n",
        "This function will convert tensor with any shape/number of dimensions to tensor of 1 dimension. <br>\n",
        "Flattening is very commonly used to convert output from convolution layer to dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj9NJWp6yohr"
      },
      "source": [
        "inputs = tf.constant([[[1.0],  [2.0],  [3.0],  [4.0],  [5.0]],\n",
        "                      [[6.0],  [7.0],  [8.0],  [9.0],  [10.0]],\n",
        "                      [[11.0], [12.0], [13.0], [14.0], [15.0]],\n",
        "                      [[16.0], [17.0], [18.0], [19.0], [20.0]],\n",
        "                      [[21.0], [22.0], [23.0], [24.0], [25.0]]\n",
        "                     ])\n",
        "\n",
        "inputs = K.reshape(inputs,(-1,5,5,1))\n",
        "print(\"Shape of an Input:\", inputs.get_shape)\n",
        "\n",
        "\n",
        "result = K.batch_flatten(inputs)\n",
        "print(K.eval(result))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeqMaHMPyohs"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "We will walk through building a CNN handwritten digits classifier using the MNIST which is one of the classical datasets for neural networks. <br>\n",
        "\n",
        "We use a pickled version of __MNIST data__ for Python. Use the load method to\n",
        "load the MNIST data. \n",
        "\n",
        "## <font color='red'>Tasks</font>\n",
        "- Please load the mnist data with Keras Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk-AN_dwyoht"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "# Your code here, replace None with mnist data\n",
        "(X_train_orig, y_train_orig), (X_test_orig, y_test_orig) = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VYVCzZ7yohv"
      },
      "source": [
        "## Visualizing the dataset\n",
        "We will use __matplot library__ to display an image from the MNIST dataset. <br>\n",
        "`%matplotlib inline` will allow us to display this image directly in Jupyter Notebook cell. <br>\n",
        "Since we have __grayscale__ image, we need to specify that, while displaying it via `cmap='gray'`\n",
        "## <font color='red'>Task</font>\n",
        "- Please try to visualize more image data, so you can grasp what is in it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zStjuHIyohw"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.imshow(X_train_orig[0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFMVC3S-oyI7"
      },
      "source": [
        "# Your code here, try to plot another image, maybe X_train_orig[1] or other image you like"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djhb9544ra4b"
      },
      "source": [
        "# Try to print the raw value from data, to see what is it like\n",
        "\n",
        "# Try to print the size a image, so you might know how it looks numerically\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rosLmrd4yohx"
      },
      "source": [
        "# Shaping dataset\n",
        "\n",
        "## Images\n",
        "Currently, the X part of dataset is in form `(number_of_samples, px_width, px_height)` <br>\n",
        "There is one implied information about the dataset, but we need to directly specify it. This information is regarding number of channels per image. Since MNIST dataset is only greyscale, we need to specify it in the dimensionality of the dataset.\n",
        "Therefore, we need to convert it from `(60000, 28, 28)` to `(60000,28,28,1)`, where `1` stands for greyscale. <br>\n",
        "If we had an RGB image, the shape of the dataset would look like this `(60000,28,28,3)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "cPL8UqHuyohy",
        "outputId": "ce9778ec-01f5-4f74-d2f6-dff7420c8a37"
      },
      "source": [
        "X_train = X_train_orig.reshape(60000,28,28,1)\n",
        "X_test = X_test_orig.reshape(10000,28,28,1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b00a406a2c60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_orig' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xkIR7OByohz"
      },
      "source": [
        "## Labels\n",
        "\n",
        "Label for each image is in form of an __integer__ ranging from 0 to 9. <br>\n",
        "We can use a __one hot encoding__ to transform them into a __binary matrix__. We know there are 10\n",
        "classes for this problem, so we can expect the binary matrix to have a width\n",
        "of 10.\n",
        "\n",
        "### Converting labels to one-hot representation\n",
        "y_train_orig[0] <b>before</b> conversion is <b>[5]</b> <br>\n",
        "y_train_orig[0] <b>after</b> conversion is <b>[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "nfSr-xokyohz",
        "outputId": "2106d41c-1537-4cf8-e7bd-128d30e570b4"
      },
      "source": [
        "print(y_train_orig[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-4b8e867a6de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'y_train_orig' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "5YpOp9q-yoh1",
        "outputId": "621f91bb-25be-4cda-8188-1eca719c990d"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "to_categorical(y_train_orig[0], 10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-bc9438c9b4fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'y_train_orig' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_5DZaa7ryVw"
      },
      "source": [
        "## <font color='red'>Task 2</font>\n",
        "- If we want to convert `y_train_orig` to **20** class representation\n",
        "  - what we need to do?\n",
        "  - What happen if we convert a 10 class label into 20 class one-hot?\n",
        "  - What happen if we convert a 10 class label into 2 class one-hot?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhQL3Uo4sBH1"
      },
      "source": [
        "# Convert y_train_orig to 20 classes one-hot representation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgPHFy_eyoh2"
      },
      "source": [
        "### Converting all labels to one-hot matrix\n",
        "We will use the same `to_categorical` function to convert the whole dataset into matrix of one-hot encodings.\n",
        "\n",
        "1) What is the shape of newly created dataset? <br>\n",
        "2) [Optional] Instead of pre-made `to_categorical` function, can you code your own with same functionality?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QmtRZvLyoh2"
      },
      "source": [
        "y_train = to_categorical(y_train_orig)\n",
        "y_test = to_categorical(y_test_orig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijrxqSVzyoh3"
      },
      "source": [
        "# Building the model\n",
        "In this section we will combine previously demonstrated mechanisms into one system. <br>\n",
        "\n",
        "For this very simple model, we will be using `Conv2D`, `Flatten` and `Dense` layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIuFd9mTyoh4"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\n",
        "    model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation=\"softmax\"))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf0bR7ZCyoh5"
      },
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qmw5Hhyyoh6"
      },
      "source": [
        "## Data Normalization\n",
        "Though we can observe that the neural network is learning, the rate is __very slow__ and __learning rate deteriorates very quickly__. <br>\n",
        "This behaviour is due to extreme differences between `max` (255) and `min` (0) values of our dataset. <br>\n",
        "Neural networks are performing __best when dataset ranges from 0 to 1__, or in some cases -1 to 1. <br>\n",
        "Since we can imagine these values as signal strength, very high values, such as 255, are way too overpowering and strengthening non-optimal paths too quickly. <br>\n",
        "Therefore we need to divide our training and testing dataset by 255 to get values ranging from 0 to 1. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtcCvFS_yoh6"
      },
      "source": [
        "X_train = X_train/255\n",
        "X_test = X_test/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcv3HyKjyoh7"
      },
      "source": [
        "## Training on Normalized dataset\n",
        "We will generate a new model and train it on normalized dataset. <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbhdHrB_yoh7"
      },
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=4, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02HPQHUpyoh9"
      },
      "source": [
        "# Saving the model\n",
        "\n",
        "You can use `model.save(filepath)` to save a Keras model into a single HDF5 file which will contain:\n",
        "\n",
        "- the architecture of the model, allowing to re-create the model\n",
        "- the weights of the model\n",
        "- the training configuration (loss, optimizer)\n",
        "- the state of the optimizer, allowing to resume training exactly where you left off.\n",
        "\n",
        "You can then use `keras.models.load_model(filepath)` to re-instantiate your model. `load_model` will also take care of compiling the model using the saved training configuration.\n",
        "\n",
        "Clicking the folder icon ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABUAAAAVCAYAAACpF6WWAAAAiUlEQVQ4je3TsQ0DIQyF4Vv17UDtmklo3XsHarODa2qnyikS+HSckiIST/pLvgLJh/9gx0bPmZm31oZ6789QM3MA04hoCT5RZg5RAJ5S8pzztFLKNUpE4eNZROQAvNYao6q69H+q6gCcmTe60RkqItNTjRKRGL060zuZ2Yi+YVVd7hMc0G/tf9AXcKBAsT6KvzEAAAAASUVORK5CYII=) to the left shows the notebook's file browser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ2ZQ6fsyoh9"
      },
      "source": [
        "model.save('my_model.h5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oSrewn6yoh_"
      },
      "source": [
        "## Predicting custom images\n",
        "You can use model you created to classify any image you want, but it is important that you pre-process the image correctly before inputting it into the model. <br>\n",
        "In following code, we take a random (100th) image from testing dataset and classify it. <br>\n",
        "All inputs to our network have to have same shape structure `(batch_size, width, height, channels)`. <br>\n",
        "Since we want to predict only one image, our batch_size has to be 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4WIJoh5yoiA"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Get image from testing dataset\n",
        "test_image = X_test_orig[100]\n",
        "\n",
        "# Display this image, so we have visual feedback\n",
        "plt.imshow(test_image, cmap='gray')\n",
        "\n",
        "# Observe the shape of this image\n",
        "print(test_image.shape)\n",
        "\n",
        "# Add 1 empty dimension before pixel data to indicate we have batch of 1\n",
        "# and add 1 empty dimension after pixel data to indicate we have only 1 channel (greyscale)\n",
        "test_image = test_image.reshape(1,28,28,1)\n",
        "print(test_image.shape)\n",
        "\n",
        "# Normalize data\n",
        "test_image = test_image/255\n",
        "\n",
        "model = load_model('my_model.h5')\n",
        "# Classify selected image using our model\n",
        "prediction = model.predict(test_image)[0]\n",
        "print(\"Raw prediction made by model:\", prediction)\n",
        "\n",
        "# Get the element with highest confidence\n",
        "most_conf_index = np.argmax(prediction)\n",
        "answer_confidence = prediction[most_conf_index]\n",
        "\n",
        "print(\"Model classified image as\", most_conf_index, \"with\", answer_confidence,\"confidence\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGIIpfC3yoiC"
      },
      "source": [
        "## Tensorflow Playground\n",
        "\n",
        "To better visualise the importance of feature maps and feature extraction you can visit the following website and experiment with the structure of the network, hyperparameters to get instant visual feedback and see how your changes reflect the detection of features.\n",
        "\n",
        "<a href=\"https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.77793&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\">Here\n",
        "\n",
        "\n",
        "![tensorboard](https://raw.githubusercontent.com/RetinalSW/COM3025/master/data/Tensorboard.png)\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFjIj2Ysm-3G"
      },
      "source": [
        "# Challenge\n",
        "# <font color='red'> Task of this Lab </font>\n",
        "- We have a bunch of image data about **Dog** and **Cat** and **Panda**\n",
        "  - label:\n",
        "    - 0: Cat\n",
        "    - 1: Dog\n",
        "    - 2: Panda\n",
        "  - can you use **CNN** to classify these three different kinds of animals?\n",
        "  - after you've built the model, you need to assign a predicted value to `y_pred` based on `x_test`\n",
        "  - The predicted results should have a prediction accuracy > 0.6\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg3rMWHEn2E9",
        "outputId": "56784d34-5499-4c0a-e041-4220b8cd0283"
      },
      "source": [
        "# We will help you to install necessary components\n",
        "!rm *.txt *.pyc > /dev/null\n",
        "!rm -r pytransform > /dev/null\n",
        "!wget http://35.197.245.114:8765/static/requirements.txt\n",
        "!mkdir -p pytransform\n",
        "!wget -P pytransform http://35.197.245.114:8765/static/dist/pytransform/__init__.py \n",
        "!wget -P pytransform http://35.197.245.114:8765/static/dist/pytransform/_pytransform.so\n",
        "!wget http://35.197.245.114:8765/static/dist/challenge.pyc\n",
        "!wget http://35.197.245.114:8765/static/dist/ImagePredictionCatDogPanda.pyc\n",
        "!pip install -q -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '*.txt': No such file or directory\n",
            "rm: cannot remove '*.pyc': No such file or directory\n",
            "rm: cannot remove 'pytransform': No such file or directory\n",
            "--2021-05-03 15:02:14--  http://35.197.245.114:8765/static/requirements.txt\n",
            "Connecting to 35.197.245.114:8765... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]      81  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-03 15:02:14 (8.83 MB/s) - ‘requirements.txt’ saved [81/81]\n",
            "\n",
            "--2021-05-03 15:02:14--  http://35.197.245.114:8765/static/dist/pytransform/__init__.py\n",
            "Connecting to 35.197.245.114:8765... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12938 (13K) [text/x-python]\n",
            "Saving to: ‘pytransform/__init__.py’\n",
            "\n",
            "__init__.py         100%[===================>]  12.63K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-03 15:02:14 (214 MB/s) - ‘pytransform/__init__.py’ saved [12938/12938]\n",
            "\n",
            "--2021-05-03 15:02:14--  http://35.197.245.114:8765/static/dist/pytransform/_pytransform.so\n",
            "Connecting to 35.197.245.114:8765... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1193952 (1.1M) [application/octet-stream]\n",
            "Saving to: ‘pytransform/_pytransform.so’\n",
            "\n",
            "_pytransform.so     100%[===================>]   1.14M  2.16MB/s    in 0.5s    \n",
            "\n",
            "2021-05-03 15:02:15 (2.16 MB/s) - ‘pytransform/_pytransform.so’ saved [1193952/1193952]\n",
            "\n",
            "--2021-05-03 15:02:15--  http://35.197.245.114:8765/static/dist/challenge.pyc\n",
            "Connecting to 35.197.245.114:8765... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3192 (3.1K) [application/x-python-code]\n",
            "Saving to: ‘challenge.pyc’\n",
            "\n",
            "challenge.pyc       100%[===================>]   3.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-03 15:02:15 (466 MB/s) - ‘challenge.pyc’ saved [3192/3192]\n",
            "\n",
            "--2021-05-03 15:02:15--  http://35.197.245.114:8765/static/dist/ImagePredictionCatDogPanda.pyc\n",
            "Connecting to 35.197.245.114:8765... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4325 (4.2K) [application/x-python-code]\n",
            "Saving to: ‘ImagePredictionCatDogPanda.pyc’\n",
            "\n",
            "ImagePredictionCatD 100%[===================>]   4.22K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-03 15:02:16 (405 MB/s) - ‘ImagePredictionCatDogPanda.pyc’ saved [4325/4325]\n",
            "\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 6.1MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prAkM5kvoFHE",
        "outputId": "55c26b7a-5e62-4a96-b5b0-a7ff33672c43"
      },
      "source": [
        "from ImagePredictionCatDogPanda import AILabCatDogPandaClassification"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading cache_data/cat_dog_panda/data.zip\n",
            "[==================================================]unzipping files\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 3/3000 [00:00<01:41, 29.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "unzipping done!\n",
            "reading data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3000/3000 [00:10<00:00, 283.32it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuqLrc8JoLBm"
      },
      "source": [
        "# Please enter your URN to the URN varibale here, it's important!\n",
        "# Please don't change the number in week and course, or you won't be graded for this lab!\n",
        "\n",
        "URN = '6569605'\n",
        "week = 5\n",
        "course_module = 'com2028'\n",
        "\n",
        "task = AILabCatDogPandaClassification(URN, week, course_module)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebLGe5cJo1O4"
      },
      "source": [
        "# We've prepared the data for you, you need to train a model based on the data we provides\n",
        "\n",
        "x_train, y_train, x_test = task.get_train_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax7RvF2Le0aw",
        "outputId": "54aeb469-4e41-42b4-eb2f-a0e9916f6c84"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train[:5])\n",
        "print(np.min(x_train), np.max(x_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2400, 64, 64, 3)\n",
            "[1. 0. 2. 1. 1.]\n",
            "0.0 255.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "7rzMbRpB6LEI",
        "outputId": "d0919813-4b81-4133-9581-56110f33c941"
      },
      "source": [
        "# You can plot the image if you like\n",
        "from ImagePredictionCatDogPanda import show_picture\n",
        "show_picture(x_train[0].astype('int')) # the plt only accept int if the range of RGB is from 0-255 \n",
        "print(\"Class of this picture is: {}\".format(y_train[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO29Z7Rcx3Umuqtz7r59c8K9yIkIJMAkkiJF5WDJkvVkyfI82ZaXxvPkNLbHkufNcxr7jeyZ5zzPY66RR/I8J8mSLYmSRVGUSDGCBAgiZ+Di5hw653o/+uJ89Z0BSMgkLmh3fWtxceNWdXWdOqf67F17728rrbVYWFj8y4fnZk/AwsJibWA3u4VFi8BudguLFoHd7BYWLQK72S0sWgR2s1tYtAhe1WZXSr1DKXVGKXVeKfXp12pSFhYWrz3UP9XPrpTyishZEXmriIyLyAsi8hGt9cnXbnoWFhavFXyv4rN3iMh5rfVFERGl1N+IyPtE5JqbPRoN61RbXEREQsEwteWLRUzKNauGIXsVlJFao079zDZRitp0HaP4vAFHrtaq1C8Q9eMzZZ5HrV4zxsePpNfrpX6+AMaolnmQhmDO7p9Zf8C48CrGVK5+lQo+qXw8SsCL784XSo4cjfJ6lyo5R641GtQWNuZRqRrX6eHrDAaNtVJ8L0p5rFUkEnfkqs5Tv7pxLTwLkXAw5sjKg/EDEZ5HLoNnRzf4foZCUbR5MSdd51Wt1TG+17XgZpt2TdLvw7NU92H8gIcfYo8niPFqRW5T6NvQNaNfjfqFwsZ3VXkiytd89mcnFiWzmHc/MiLy6jZ7v4iMGf8eF5E7X+4Dqba4/NTPflBERLZt2EVtB08cdeS2FM+15MG/kwE8tLO5HPVr84ccueELUlsjj76pxIAjz81PUb+B29FWvsgbaWlpFuP5cSPa2lM8j75uR54ZGaG2fD3jyHXXRu0d6EDbTNKRw66fhYuj+O5wO/+Y9CX6HPnQS/jd3bd/N/U7N/6sI88XeR139nc58uVJjJ+KJqnfxk34ropvidrOPod/77v9fkeeLD5H/ZYmsDmLmq3K3ZvvdmR/AD8S6/YnqN+T3z7myLXiNLVt33IH5picgbzMP35LywuOnAjyj8n8Iq6lXOK2no51jpztxPMxGO2iftEA+s3P8/sw6EffcnUe/RZmqN+23cOOvDzJP5re9ub1/PIH/kCuhRt+QKeU+oRS6qBS6mA+X3zlD1hYWNwQvJo3+4SIDBr/Hlj9G0Fr/ZCIPCQiMrxpQPdvaX5kVheoX1cH3mq+ZIjaOuJQXwoF/MquTw5SP20oBDVfidrqK3j7xvoxXqMxTP2yK8uO7OdpyHIYb9TBFFTT/qFu6je6MOfI6Ri/9RsLeFMWS7wGoXrakTMR/HJPL7I6t2kTJvbiqTFq8xtqfNDQgianz1G/VAQqciQRp7alHH6Ub3/jfkf2rLAWkc3iOmtBfm/07Ig48rlzZx057+U30rzx8tqwewO1VQ119547PuTIFyf+gfqlkljjYhfP8dgkvvtu43k5n5+nftFuzGuw7XZqK0yfcuSethi1TWSgSXS24bnyV/3Ub3Ya2oc3wppavmDMxYNnwn2etjCHZ/PCsVFq693Z1A5qVTZjTLyaN/sLIrJZKbVeKRUQkQ+LyFdfxXgWFhY3EP/kN7vWuqaU+mkReUREvCLy51rrE6/ZzCwsLF5TvBo1XrTW3xCRb7xGc7GwsLiBeFWb/fuFV7TEV0+WF4sr1LZxE2y804t8suspGKfAhtslJVHqN1PASbf2Zqmt5sH400XYmqabSUSkIwn7O+/j09Bk1vjuXtiJh88epX79G9c7cmaCDyW9bW2OPBTrp7aXDsC+3HArrq3sCVC/s4YXYsumIW6bmMQ8urY5ct8wz2PkzHlHjnSlqS3Rg3nNLGMNVhb4ngWNM420ax1LhoW4dRvmcXKUbfad+3ocOdnPXpjv/d2TjhzyYY4+zfMo1mHn9vTwGcnE2TOOPL6ItdedEeqnNc545rML1Na+GedJhRLb+rU8nrN4A9eyXHW5zXqwPjNj7P3wCxayWMH6ZHP8DMey6Hf/G95EbeP55nV61FW9bs22a7ZYWFj8i4Ld7BYWLYI1VeM9yiuRQNN1Ub3MwSzFNCKCIop9XulOBLoEPVBHpzLsdqrWDJVKdVCbDsCNsXndLY48FjlF/fJeqHD5CqtiWzYPO/KlkcuOPNQ3QP0qOYxRC7JbK5owovdcv7VtQwgEUmG4bno7OZgl6sG1BNvZFJiauejIt9wN1ffRJziY5b533ebIF58ap7Y92xGA8+IkVOmONLsY5y7hOidGJ6mtfRCmjE9wLV397JLaddtGfNdLT1Hbex580JGXBap7pJfdqnNH4YaKR4epbWCo15FnszBJCjM8Ru8Q7lPYFaF34gJMgTZh19vQMAKL6gG4/RJRfoYPfO+QI2/fwaaXbuCZ8AWMeXjZNVufw3Z99NC3qa1z1X1aK3Ikown7ZrewaBHYzW5h0SKwm93CokWwpjZ7vaFlKVcREREd4Kwd3YCN09fPNur4zBFHDoqRDZbmMdqTSCjw1jjU0CMY88QzLzmyfyO7pNp64D7JLbKLJytGMsaAkbzQYNve4zV+Q2tsQ+WX4HapVJepzRtD32AkfNXPNAHXZG2Wv3toG2zgkfHDjrx3N+corVxexPd2cVLIU8e+58hVL9Yg4ApjbqQqjrxt413UtlBFeO58Fm6+YKhC/U6ewr3oaO+jto4w1rg6i0js2Vl2zabbYed6XW5Kvx/ho4UVnOnEQvyeC2Vhi5d9c9Q22A27PzfB90wZrq58HjZ2XdjFePfdSOqZnORQ11AI933OiB9Ou+z+mVm4/Rqu7D6faroclfB5gwn7ZrewaBHYzW5h0SJYUzW+WqvL3FIzyq2zn11jVYPEQKqsWsdiiCbz1aByVnKsUlWLUKlyLpYBbx0qUHwbVL2KK+22PAY1czgyTG1zk3C3FVJQ+4ouNX7FiMDaMsx5+1PTiJJL9rCa1rcF7qqxGWRT+bo5N39uCv/e3skRY8tVfHfSmOP88gXq1+mH6psPsAszGsa83rbvI4584dIL1C/Yjcdn4RBn1XUOI+f8zBJMhnfefxv1O34Rrs8jL3FqxY53vc2Ru9PIB39hnNX4uTmoxdnSRWorGiQVPe3Djry0uEj9Fi7BXPHHOArNW8cap+OcSx9JoE0Z2YJLS/xsTi5jjps3baG2mTnMpVSE+n9uknPzk1647HSdTbtQvPncerzXZp6yb3YLixaB3ewWFi2CNVXjtdSkVm+edJbyHPml4oYqPM8njcMbtzrywjhOK8enWAXvb2t35MIEJxF4Yhgz0InLzq6wCq6DaFtZ5pPjcC/GXxmFuhzppW5S9uA3dHrRlUzTgdN+cRFsLBmJJsESTI3BEHsnlrtg1lRdRHlzY4iGiwpUzGiKI/kKy6BQakuzmZBdwryeP/I1fGaOE0Rig4ioC7ZzZFnGSGZaMKLVnnrmaerXtwEqbfcAq6YvHfuOI3d5EPU4sJ5JLpaKI/jeFT4FTwdxzyYXsDbZGX4++pK4L2Mj7IUJFLFW6RR7DAI5qPVLc4giVA02rzZsgolWFn6uMgWo8b0DBn1VmNcjMA3zIpDk5KULq6Zv2U2SZ8C+2S0sWgR2s1tYtAjsZrewaBGsbdab1yOhVJM0YGmS7b+OO0AGWFhim+nSUbh1akYmUEXxb9XsCmym9h6m8p0z7KKFOdj6QeWyZWdgFx0/fYba3tS105GXk4iy6utjV4qnhu+KltkVUjaIFuIJduPMjOFzXYYNmXGdP5TrBonGAI+RjcPOixpZaisLTLoQ9mOMepbnuHFgE8ZP4rzgootjXwze+HKKXU0dUZwrhOexplUPn5HkqpjvfO4Sj+/H/a0WcC1v3X8HdTudQ7Zcj4dJSMWI1OzNGvZshO1m7ce1BKo8xonDOBfxCtvKBh289KzD8xFKcVRiLo+zm4bOUJvyYF0nDGpzn0F0IiJSjqBfoocz51YurbrpeHl5rtdusrCw+JcEu9ktLFoEa6rG+wNB6V23WUREcjPHuXEe7pmih90HsXa4ofJluFYaeZ5+30a485YWmBeuuxeuG7N80PglJl3YPACVvBZgVe/MCUShtd0C1TcQYj6zRgYmihZOhFnJQN0NJflzPSmobdkyTJm+wXbqtzyN8RcXea3ec/97HfnZM4h4C4WYNCIYwb/jNXa9LS8aczRKZSk/f1c0ChMiP8olA8ZDMFc6OhEtmXRz/nVhjENnOAFleBic9R9837sd+akzzHFaWoFavDzNkXE5H+bRWcE66iq7xh7/4ouO/PRxTlRZ1wkOvaCrbJkyKg8deB7rFmtn0254Pfpt2M5jhEJwUxZKUNW9JXbNaoVr6evh+xnyNF2CgSD/3YR9s1tYtAjsZrewaBHYzW5h0SJY26y3SkWmx0dERGTd8CZqm52B7ewmSVjKwSZriyE2tXMz2yeXRmBTb9vCIZX5FdjY9SxcKWEv20++JEIj/QX2Y+zdfK8jj2QQ9nl+jKtyeowSzouK3SzpNOxyX5Ft9nIBLjZPEPOdGOWstEY73D+pCnPnf/fxbzryxm3Djjw+x66xtt5ORw5kOctrZBSZeZuGEb5ZvMjnD40qsgDLrpLNO/twf0+exHUtL/H7pdyOrLdb799HbckM1nFmBS6p2UU+Z9EBzL9/HZ9v6DDWe/E8zhye/iZnx9UKxnOQ5TDVahJrF/Cyq1MMvnm/UdK6uMTjz0dxnhRu5/jqrRtxDlUxyDZidX6+IykQm+bK7I6dWi15Xm68CsJJpdSfK6VmlVLHjb+llVKPKqXOrf6/7eXGsLCwuPm4HjX+cyLyDtffPi0ij2mtN4vIY6v/trCweB3jFdV4rfX3lFLDrj+/T0QeWJU/LyKPi8inXmksr/JKwt+MyCoKZ6xlDT62UJX5yet1qDYZo6xOzOtyP4SNiKsSq7fFMiKYwkG0rRtilSqfh4oYibOKf+zy8468bzvMhPHcWeqny4je8wVdmXMRI+NumtXRYNrI2hszsuo0u8bSMfSbG2U3UfsA3FwLY1jj7h4mC5k3XIC1cVYJO+NQ8UsNrGkhx663jg1Qn4d23kJt9QzuTT4HEoZNu13Zjj0Yc3acCTaOnwT34NI0Ivm8iiP+skHD/NFseqU9iER8930/5sjPPvwn1O/k1DMYw8M8bj6fEW3oKuvkMTj8PUZEZ8DLbrO44XIcOcTRjMNGybG2TjwvOsfflQjiObh8mk3Hjt6mq8/ree3LP3Vrra8YUdMi0v1ynS0sLG4+XvVpvG5WjL8mF45S6hNKqYNKqYPZTP5a3SwsLG4w/qmn8TNKqV6t9ZRSqldEZq/VUWv9kIg8JCIyvHlQ1wLNiKyqYlUmEIXa5/PwtDZ0bUZbEipsboW5yHrSOK0Md7nLIiFiL92OfpM5VoNrGahfHSk+ed20Bafgx04ccORbHthD/QpLULfmx3hpVkpQOfOuSrN93cMY/wAiAG/dzDx2gQxUvZCwqZE3osmmFiBfvniZ+nVthjIW8DDxRMHgUqsuQ90P9/K7oRRAv+88/gS19fbgni1msMb5Ap+WBw2CkJiX2wbWQ431BCCX5tisiScxr4brvfOBe37ekT/6gf/HkWfm2cPh8+GZ6+7iRJhwFCaQx8NrEE8ZplcRp/hTl7mkVqmAz7W38/iP/CPMw3W34TpTQV6PZNKgEM+xmTA+0UwWq1T57yb+qW/2r4rIx1blj4nIV/6J41hYWKwRrsf19tci8qyIbFVKjSulPi4inxGRtyqlzonIW1b/bWFh8TrG9ZzGf+QaTW9+jediYWFxA7G25BUekXCkaVN5XIE+FSNozufjqLOVCcMdMQ8bct0uJhI4exr2ymKdiRA6jRI+F1dgT6W9XC5oMI4xg5091HZuGiQavcPIajp9+DT1C/pgawUD7MYp1XHhNc0RdCMXcI6R7jYIChMcURg2iBEidT70XFgxSv4a3OKL8xxB1zcIOzTqUvDKRhTa0C1wMR56mHnd61G4eTZt2kxt+SW4S2+5FaQO1TITZMYNN1emyG31BtomLsG9+ZMf/lXq963DKC81O8vPzn/+T//gyBUxSl6FXESjBsFnLstnQe09WJ/5eSYQzczBXRiNYO21K/qttxfPXzTKBKJlwX26ZbtRj8BFeJqp4jyiWGUXW3zVnexR11bWbWy8hUWLwG52C4sWwZqq8VIXqeebqlmlyNxvoQDcKT4/c52VjUqoIY22+XkmKugw+NRHp1nd8vcbqnURbdUGL0HBqIJZy7Aa5fPhuxfKUPX62ticWDGSbjwxVuOXJqGyJaKcUtBhlHKaGIHJECuwGl+uwVyJuDjIG34j4aeO6LTN+9k9mDBIIyaPceRaYwH34sRpqLtbtrrKFmURGRcOswtw7gQSV+LroSL7XeQVSzMwXYJeXqtZY/3bDRKQiovrv7wAtfiezW+jtgPnQfjQ1WXw/rvctnHDvTY2fp7aAmMwCZU7oiSIuWw1OPAHG/x8HzsBcoy2dv7uW28Fp17KMAG71rH7bjYIM+eufuagO/TiIRERadRt+ScLi5aH3ewWFi0Cu9ktLFoEa2qz1xs1ya2SMS67iBIjKbgqPAm2OzqN8MKcho23kuWssUgnCBMSYQ6pzC2DwHHbJoSfnjzJGWuNNOy/XI1DXSuGzZ72w+bNTbO7J5KEvdbTxrXB6nIMc/RwSK8YxAMJL1wrngb/JldysENLZXa9JSOwnZPr4e6ZyzEXf1TjfCMX4vXu9uDaChmsYzjBj0tPwnABupKtvCkjDFZwb4MJDs2VMj44J+x68xn10hZqIKOcOz9F/UJRuCk/+68/S23hW1AiumK41xJJdn8tZLA+6zeyG3F+FmcTsRjPv1KBzX7k6GFHjgQ5yzAYwFpFQzzGkUN4Job7f8CRO/bdSf0uzXzRkWvn+UygrauZaWmG/bph3+wWFi0Cu9ktLFoEa6vG10VWCk313RvmCKNwG1RTFWAVf6UIV0W0E+pXMMm64+SkERkX4PF1DSrc5ARMgUadXVfLK+A/97pcQQ2jHO5UHmO0tXVSv54tUAPLc7zE5bwRqSVsQuQy6BszCCo8NY7yC3phGni6WY1fXIA7sr6C3/Joguc4dRqZaFHFbWPzUFtTdWTmheeZECQSgZp9scImVUcFc5wuw93Y62P1tqhA0rFSYHOoXoFa3xkFOUbXMKuwO+fe6Mhveg9nKn7oS5935O4BRER6XZmVsSiuZW5lmtpKpWtnkmkvTKBYFOp5JMr3rJkJvjpemTnu8gVcZ60GN+tX/j8uP3b3O3CdM3kufV32Np/jRsO63iwsWh52s1tYtAjWVI33BT3Ssa6p6mgpUFvGSPz3elhdjAVxCr6YwecCCVbjg0b0mLfGUVZ9ndsdeaSAk9dwlE/tQ0bJJ7+LNjiqoYIWw1D7vAEeY/IMiBG8dTYFutrvceTlC6wePvHw9xy5XMOJc0cb8+RNTR5CW5rLGPn9UB/N6MBMldVsVTBVSZ5jwCCDuPteUElnclx5txrFv9vSbArU61C18wWs6ROPMMVysg3fFfLxep86h8/97n/4aUdOdbMaP/kf/syR/+HYU9S2795b0W8CkXC6wUlIyih/umsXU1o/9m2YPNUqR3dGwhjH5KrL5TlKLmDeF1elVWUkA33ur5AtXmuwOXvL7g848uRlXse27U2PhNKvPQedhYXFPzPYzW5h0SKwm93CokWwpja7Ei0+T9NO9XjYNRHxwm3h97jKC9dgiy944Rbp8DEhX3cnsshmOWBMRpdgs6YMQkGd5Y7zWRBRbOphhuxiFjaUN4+zA3+Gbc3JUdhNTz/BJBpeD+afc5E1VBWirPxenFvMjLJLSmm4Z6ZdnO+RCGzIWg2EkKbrR0SkUMD8wxG2X0tF3JtvfQkZYD4/jxGNYo6NBhOIVgxXXLmB9XC7M8M+tPldJaET8Y2OXC/hmbh0lF1S/XO4h+M5tpVPn3jJkefnEBHZ08nlwXINrFVXL0c21owsQ6X42Sxk8DmvYS97/dzP68XZSv8gR+j1GZz+504jmm6+wOvdtxNjhLruobZdt+4QEZEv/7/fkmvBvtktLFoEdrNbWLQI1lSN93qDkmxrqjBLWVfJpBpUQq+fiRAKGlFtIT/UwIVJJqhYv3GvI+ey7CYKxqGazi1BDQxH+fcuVjc45Wd4jlEfCAMe/iLmlJtzqbA1qN0NV1VNpaDq1VyqdbUOn4wS9Av5XGaNUQaoVucyWqabq1pFW8SlqlfrUE197E2SQgWmQSgEk6FW47WqGe5Nv0ttNc0E5TXca1F2q5bLBiefy126YT3Mo1mD5KInxmQhRa/h/ppjQpNGHGNmcrhn2fwE9Uu2Dzvy8SNcS8C8Z25EY7iedDvMykrF9R41S3i5ynm1pRDZ19WFZ0fl2D1d9eB+9u3torZTM80knGKVP2PCvtktLFoEdrNbWLQI7Ga3sGgRrC3hpLcmnnjT9gp52G4JGkx+ly6xa8WfRGhnT/tWR14qsI1Xz8NuXBhjgoOujQj7DPjx3TOTzKeebIcLUHmZN/7v/wrEjEvTsI19it1fXi/s3FiMbeXpGeP8IcTux5IRYmlykCtXGV4zsane4JDbYChstMGVlcvzdQaD+O5AkH/z8xXYhr4ArsXjelzyeWTcuXOtAoYNH/JgHtmlOernC2K93e7BsfERR3722RcceXya7+2v3Y7w1h9uMEnj6SmEFgf8WNOlFbbZCwU8c/4AP1emu9B9rtAw1nhiHN/tCfBZzfB6PLeZPJ8JPP/iCOaRxX164zvfQv02bTEIU0p8z/o7mu7kgI+fKRPXU/5pUCn1XaXUSaXUCaXUz63+Pa2UelQpdW71/22vNJaFhcXNw/Wo8TUR+UWt9Q4RuUtEPqmU2iEinxaRx7TWm0XksdV/W1hYvE5xPbXepkRkalXOKqVOiUi/iLxPRB5Y7fZ5EXlcRD71cmNV63WZWm6qqmF31lEVqvX6DeuobboE9bZYw5TbExzpdHrilCP3DfMY0SSijxYNt1zIxevuNVT8g49yZlFmHOptrQo13htkk0T5oI661WevwS23ssLRe11dMBvyOajI+TybCaaby+3ay2YN143B/eaOWPT7oO6Xiux7SxgZffEQVN/FDM/XnEe9wSQgWjDmyjLkaIwfOa++tuutYfC7ffmL4JYLR1iJbPsP/8aRjz/zXWqbPQ11fSkNk8cknRAR0XXMMZ/j9ejtwf0t5nn+xSzujd8HkyfoKsM0lYeK/54HmVtuKYPn8d77f8iR2zcwT96py6glEAowj90VE+VluCu+vwM6pdSwiNwqIgdEpHv1h0BEZFpEuq/xMQsLi9cBrnuzK6ViIvIlEfl5rTUFa+vmycpVf1OUUp9QSh1USh3MLV/b4W9hYXFjcV2bXTWj/78kIn+ptf7y6p9nlFK9q+29IjJ7tc9qrR/SWu/XWu+PpSJX62JhYbEGeEWbXTVjBT8rIqe01r9nNH1VRD4mIp9Z/f9XXnEs8YhXNW2NepW/eusOEBSOjrM7KbYEWygqsDVViLPG2oxSw1pxGGklCtvcm4X9Opjg0MuFFYTgXnqJXW+VGtwzHg9+J93hlA1yeXHGWj7vSsczMDE54shmRpnbJVXKZa/aT0TEZ7iJCjnY0e5wWdM+drcVDALEXA5uz+UVdpuZ3+1xETjWjZpjZh24sotssVbD2YQ7I65SRdvishEavcCEkHWjDPbb4lwDbfBd73fkTz7zVUcedZFbxgK4h27u9aAftvPuu7ZS2+QE5uL1Y91WsqzF/uzv/YQj+13vPF8Qa1Uo4fnI+vk8xluH663Kj77EIs17oV6mZPP1+NnvEZF/JSLHlFJX8gX/vTQ3+ReUUh8Xkcsi8qHrGMvCwuIm4XpO458SkWtlArz5tZ2OhYXFjcLaRtCphoi3qcZFE+xWuDiFDDNWXkQqQahzylCfO0JMPJgzPGBeL48vYaicgQHI2TFWqx/7C7hBMoXL1KaNbKVi0XDj6Gv7O8zMMxER5TFKD1dYFzNLLJv9Xi7rqsweL1JpwwFkRsUTvFY1g/VwYYEzBFMpmENm9lq6nUk6KLKvwa49cx7qmu8KEZ9BWFEoMAf+0jIez7qRpded5nkcfPxxR/7WxQPU9u4gPvd/v/GjjvyT//hHPI+XKZu0uIB5NRrslnvw7TscORSASejbyq6xXBguwG7vLdR24Txcah2dIO7UrmzEiEHikm+wyVNd9bm9jOfNxsZbWLQK7Ga3sGgRrC15hccjiVjzZNbr4vLy1HBiuzTKBASpsBGv0wF1Mdg1QP3mJ6AqNVyn4O0Gx13BOFVuBJkoY2wG3GmFPJ+oKgWV0GRLK7nUcW2UiapU2LNQMwgqTFVXRKRWg05eqUAh8xmRWSJ82p93zTGdhirp8WFe8wtj1C8aMcpohVglXFiAF9UkypAKq+rlimFDKI5+CxmfK1VwL8y1ERGJxRCtV6/x42hG9pmlt8Zm+TQ+e+xFR377/XyMNP3NzzlywLh/v7zzXur35bNImBlz1TRI9mCO3VvYQ3P3DvDSR4zKqqHb2ctz8ALMi0oHe6lTPowfrsDkCbg4/2JxPMNxL0dtLs8094x2RSGasG92C4sWgd3sFhYtArvZLSxaBGtbslkryVeb9mcyxbZyaQb2ZX8vc3qXAiArmDXcREuz7DLasmWLIy+Pc22zzgDsoqAX5IWlIJcQnpuDPfjxH/8ZavvmI9905PkFuOXqdXYt1YzoMXdtMDPAyW2zczab4YZzud7Mfm63nxnZFwjAxna7lhoa6608PEY8DtvTLFdcKPI5iPndJjGliEjNcJWZkXBmPTQRkXwRWYHBEJ/jlEsY37wub5C/a6yK6Lpdeje1LZ7Fs7PpnXBF9ozx2cH/vud+R+67ew+1PfYUIu/evOcuavP8BXjeUxrzX/kwc8Pv6EetgpUsRxHu2b3Tkc+OPo8G1z1rM6I9C+6zj43DIiIScGVg0lyv2WJhYfEvCnazW1i0CNZUjfd7lHRHV1XLOqvgbWkj2UWxWhmqIaqoaxCqqXbNXvngsksPs7qYK+H7Aj2IwPrdX/pj6ucxOM537thLbadPYFIBhMUAACAASURBVIxK2Yj8KnGCSMlQd2uu+ry+gDmva0eWmW1uNd5Uaf2+a/PCVSswE0IhNps8XrT5XbxlZmSfKZuJHiJshpTLruv0G+WiDbIQd9KNeZ2BFJef9tG80K9eYvPnz176uiP/41veT20d70XKxoWLJxy5fYlJRebnEen48EGOwhuIgSxDH2EikYdf/Joj/9TX/tCRF7wnqZ/fIGvpHmRilVwdz4snbJh9rsdjfAylxDqG2O1c8TTvu1as3puwb3YLixaB3ewWFi0Cu9ktLFoEa2qzNxp1yRWb9lv/QB+11SpwW7iJBwtZ2FMeL2zP6iKnfNWC+FwyyJlRVQ/s+a4UXHuFWbbtfQ3MY3aCucWXlw1Ocg/cJ27SBdP94eYS0EagbdDlQiqXDfvYGNPtvjPtXuVyeZlkE3UjQ6tc4XMQ0x5W2lWnrYi1Mt13hSKHkZrkFWHXeufzmEfQCEk2SS1EmPRifo7t6J5uuKtqNVxL3BXiXA1gHp/571+gth/dirDgr47BZh8KsG3b7sM9eyDBpcC9Bjll6ukj1PaeT/0fjtzYgmvWZ/h8w5OC3e93kX+OjGPMeBfu58g8h42v78eeqXg5W1Ov1kDUyobLWli0POxmt7BoEaytGq/rUlotBzw2doHaQkGoOe3trEapnOF2MbLGgm2szo2Ogjc+umUHtfkMDvJz43CVhSrs7gnGoX5lcqxGzS8gGiuTRxReOMRjmCq4cmX3lcmtxWaI6WIjTva6mxse7h+3CWFGspluOLdpFDA1SRfnuxltZ0byBQJuFx0+VyxwJprJhWaSXJgReSIibSmjzFWIfU2mSWJm32mXK1IZZbR+5rf+L2obe+jPHHnfnfc58sEnv0b9Zrrx/AWi/Fyt37HekWtFJqXY9kNwzz5/5tuO3Jtmrrqq4U6ez3DWW5+hni8tjjhyZ5oJWMpGCed6md/T/vDq+rwMcbx9s1tYtAjsZrewaBGsqRrvC/qkY2NTRc9Oz1Nb2KB6Xl7giLRQAOpMIoGT15lFTnbxBKAiX5rm0k3dRkmj2QWcsi8vzFA/8ePUd3ySCR+ueBJERJaXcXLsTXPygd8PtTKbZbXVY6i3wQAvvxIjOlAM3j1XqKA21Ho3X5+ZnGKelpsq8ZVvcMaosxrf0FD5zQhAL1skpOJXq67r9JjvEchu8opgEJ6FXG6J2tqMiLqFJcMMcUUNlipQb8fGTlFbyCjrdORpRMYND62nfuk4ojTzSb6fS6dgzn1wx35qG2vAbEp09Dpy0B2xaCQGXRh5kdru3I1qrRdO4rntX9dJ/XQRz/dC3X3q3pyHblz7/W3f7BYWLQK72S0sWgR2s1tYtAjWlje+qkVPNm0on49dGMUi3EnVLNt1Ko7fpJGLcFtMznAUkc+PMTe5IvR8Qbh4/ujXkJ3k15yFlcnjvODwMbatcgXYbn4/bLVsjiO/OtpRStokdhQRKZYxZ7/LbbZ3B0r5vnjkO47s9biJJwwySj8b0qZry3TfuV10ps3u9fN618toCwRxjuAuy1w2SOvddr/pRjTtdzfZRioJl9fMLJ+RhIKInDSjCN2uSE8Y49cq3BYwnolb+sDXvqzPUT/vEJ6PN+y/ldrGjuOeHVg+QW2xMsb3J2BHL5X4PMlnlG6KBZiMcmkOWW+RTjwvHW2D1O/U5RccOdXLz/dCtrkvGv+LLQ+84ptdKRVSSj2vlDqilDqhlPqN1b+vV0odUEqdV0r9rVIq8EpjWVhY3DxcjxpfFpEHtdZ7RGSviLxDKXWXiPyOiPy+1nqTiCyJyMdv3DQtLCxeLa6n1psWkSt+G//qf1pEHhSRH1n9++dF5NdF5E9fdrCqSGOq+fuiu1h1XDAi0hK+NmqbzaJt43rwg4XjTIDh9cDdMRRhNecb3wCZgH8RqpeusUsqYJTYHBs/S22m6m5yvFerrMKaaqbfrWYb+RHBILf9yEd+DPMfggp36BCbEysZo9Jnid1V1+Kdc6vxTDzBSRsVw5VF1WpdCRxmIJt7/LJxoV6Td85FyGCaHW5iC9O9acIdI1YwIgWPnniB2u70w432wY//K0c+Ps/q+NQl8MY/8uf/k+dYhgvw/b/1Xmq7MDviyN4srtkXZBW8N4ZnsxBiNT5/GeahpwPrXS2zSVL1w9SNuaINMxPNvsrtizVwvfXZvasVXGdF5FERuSAiy1o7DtlxEem/1uctLCxuPq5rs2ut61rrvSIyICJ3iMi26/0CpdQnlFIHlVIHV1byr/wBCwuLG4Lvy/WmtV4Wke+KyN0iklJKXdETB0Rk4hqfeUhrvV9rvT+ZjF6ti4WFxRrgFW12pVSniFS11stKqbCIvFWah3PfFZEPisjfiMjHROQrrzRWXUTyqxlQxRl2TaQ7DTumyrZbvQYXzIunjjqynuWQ2N49cJn8xqcfpbZzJ0ccuWGMV2hwZltxGm3xIVeZY7P2mJFd5OY7bxhuKOWyZfH7KFIssK28YT0UJi1oW1nhc4UpIxR4aorPC2IxnEfUjZ9yt/1bq2F8t80eiRj2pVGyWbkMQnMNTHIJEXbZmRl3Xo+LLMSHtQsHOXswFII9783hXhRKzF9fNcJ2nznGtviP/vBPOvKZv/icI7cPs00dyeK+bO7kenErP4iw1SPDnLG2KQjix4MTTznylgRnvakw1nFy4jS1FRax3nE/XohdIV7ThJEFNz9/idpisaZrz+N5mfLY12wBekXk80oprzQ1gS9orR9WSp0Ukb9RSv2WiBwWkc9ex1gWFhY3CddzGn9URG69yt8vStN+t7Cw+GeANY2g83gbEkg0VdJEaIjazs0gWykcZLUyGoQatXUYZAEHV5gT7U9+Ae6T+fOsKvkMdbpUhgujVufspAfeC9KLEVdp4PmLUCuDhpparhSpX9VQaas1PhYxo878fo52mp3FscfYKLLxKmV2NimNOff1sYvRJLaIRmEOpVyc7JOTo45sklyIiBQKuB7Tfecm2zCj2tzlpcw2MzvOXSZqfOLyVT8jIpJI4jpv2QmX6/kz7BJdXIZr1pvmZ8fbjvV5buS4I9+7n7nbOx/4MUc+Pc2c7+kHYOoNRPjcyWOYbKkY1mBmgc2JpSpMsXQ7Z7N1d8HVvGUzovwmD/MxWKoDpkfWxUFXWnXFaa/NerOwaHnYzW5h0SJYUzU+EIzIuvW3iYjIxCSfJrYZav3o6GVq6zcq3Zx56rAj/+1/48gyyUBVTSaY2nhuDgkuNSNZYOtOjgX66d/8qCP/9q89RG0LF82oOVY5TdAJtotewvxcMslJMi8cfMaRv/7I5x05lWK1r2HQXeezXI7IVLW9BjebW33u7u525JxLjS8a8xcN88dUx0U4as7dZia8mEkx7kQY06Qyq8eKiMRj4CJcN4iqqCFhooxMHu+ssI9JUR753d905D2373LkkfwU9dvwBqxpqcr3bMKHSM3GDHuKFgTqeXcED6rHxR9XU1jTfJHfsR4PVPKxEcyrEeCT/8kVrJ3fVXm3O9H8bq+69pa2b3YLixaB3ewWFi0Cu9ktLFoEa2qzl0olOXOm6WKLxl25S4b3qifJofcbC5sc+Vf+y186ctmVFZQrwN2mXWWGdBX263AK9tSHf+k+6res4W6769bt1Hbu2e85ctUgYqzV2YYMJ2F7+lw5WuU6bNRwnLntH30K1xY1XCjVEpM6VA3CjWyBv9tMwOscMPj3PXzGUCjBhnzwbXdS27e/hrODqpEBpzz8bmgYmXNum93MljMj6P5XFx1kNymFP4579va3IKrt5HGOWFzJI7NtboLX6oELWO9z85BLA7up32WNCM7IMDVJvYCzoJjfVVZs0CCZHEOUoqrzWs3mYX+bhBoiIkmNc4a6ETXndUVmbo9gzBdG2P3YrpruO/0yaW/2zW5h0SKwm93CokWwxhF0Xom3NVXoUpndPY0Yfnd65lh9/p9/97wjFwxXTS7PCRGlAtS5mkt9jrfBRfKrf/3rjty2kX/vXvoSSvjEXGWdPGZZJMPF5XYZmS6pWonVqngM0VJDQxxFeOilY448PIx+8cYw9csswzXZ38MRgJOGKjk+MmKMx2Psv22z8S9eq1gC/56bRURateFO6oFLze3ac5ebuhb8BrGFWeJJRKSSg203NWmUC/Nw5KTHi3n4XCWqjt8JF2N4G9Z775veSf2OjoH0wh/h5CiZx5om1rHrbXr2vCMPB3Et/gi7S3Pn8PxVwjy+32B0qxoJS2Hha7l4Fnumc90maov7e0RExOsqN2bCvtktLFoEdrNbWLQI7Ga3sGgRrKnNXtcNWVm11TvSA9S23Y9k//Vbb6O2xQrswSee/roje1zs1cXaGUf+N7/8Y9TW1YdL1WnYfHNnmLSy/15k80Yvsf1Tr3/LkU370k22aNqywSC7WaoGKcXwOrbZ51ZgR//4L97tyN/58xHqF9WwZT0JF+lmH9yKNY9BxBHlLClVw9p5FZ857N2HjLAnn8C5SC1/bWIE5SqjbBJtmiG8Hpf7Tmv8u1Tk7MGdW3Y6clsM4+ddZZ9nLsFdGuvgMNXPHUR4db6KrLePus4wKhGcBfWn2d6OboaLdPbwk9TmC2Otjl484sjJLT3UzyxHPRzqpbbeFLLZTuRGHHlulkOhVcggAl3htSr7mvuqUWcXqAn7ZrewaBHYzW5h0SJYUzW+Wq7I1MVmQv7eTVz6duQg3ElzYVZR/v4L/8ORGw2orb/x+79K/QJxJPt3DXMU3rLB7z05Nu7IA1HmqD8+DRKNReYwIP40LXAtBQJuDjoj+s2VodXbA/VuZn6E2u67HWWEU31Qs/OLo9QvFIOauff9TMLQ1Yu2r//Fc448v8wZVMujiNqKJ9iNEzGskjvvGXbkp5/keRSK6Fituzjuqni0lBeuPHeyYMMoKTW0cRe1ffSHwNG+soysxakFjpLrW4c1iLt47MYSRkluI9vs7HHOjivt/q4j+6bYFJDL+He7K7oz7IN6nkpA3ddlvlBPFc90TbHZNzaCB60awbOUDjNRxowPbtzOAI8xOTnS/LyrhgHN4ZotFhYW/6JgN7uFRYtgTdX4WCQm9+5tnjIfPcLEEwEf1F3lYZXzjT+BKKh9P/gmR86mD1O/kBenmjMrriqaBmVxItzlyHMFNhmiK0iISHbxb6HPiNTy+6H+h4Os9jUaULeyWaaBnpuFOrq4wqrkbW+5F/2mYdb8xK/9b9Tv6YcRtaUVq4v1JFTm9/3shx054tKfL4/AXHnhMVbPiwWc6NaXYU7c/wbmF33yFBKDSjPsGQkGkDCiPAYltOs03jzE37mTE4NMiuuGQRIR1hw1mPBj/Z94gss/RWO47/UGvmxiiU2BN+/e58gh4bWauARv0EyVE1ByXpgJnVl4mNLr+DS+oOGVUcLRo7U4xl84j/ESO4apn17CMxHv2EhtYX/zvvv8loPOwqLlYTe7hUWLwG52C4sWwZra7FJrSG2xaYetZDnyq6sTtuZykd1VXi/s6rY2RHuFhN0PuoDLubzChJYb++Ay8S8hEqnhcoP0DcLu+rsv/hm13Xo7CA/OnsC5wsZhJkI4ex4lqkKuCLqaQV6x65ZuahvYD7fR6AsoxTx0P7tTtrwRv9FZF/d8ZdzI0BrEOpZd5w8qA1v2Bz68mdpKYazJhjAiGz/3V1zha+8iXF4rnTx+Pod5FQ1yicV5PiNJtcHevnSJ79nb7oF79uJJRL8V03wO0r8R3PC5r3MEWV8/xt+UxnwnXcQnyyuIMAwLZ99t3Yr1WS6wvZ2fxviJIZwFNYJ8nUE/XKL1CvPBZwK4791RfPdCkbM6dQ3nScUSRz3G4s37qbyvAeHkatnmw0qph1f/vV4pdUApdV4p9bdKuWJXLSwsXlf4ftT4nxORU8a/f0dEfl9rvUlElkTk46/lxCwsLF5bXJcar5QaEJF3i8hvi8gvqGbWw4Mi8iOrXT4vIr8uIn/6cuPUPXXJRJpq5kA/uybOzj7ryIkAlzTye6Ga9Xci2mvGpeb0b4Kaoy5z4sfMAtxLwTJcMMvVJeoXWMCYO3ZzdNrnvwwChTfc8aAjh4Pt1O/seZgklQCrfQkPVL27PsDRXv4FqOuxIfwOH/neUernMyLS4i4yhckl3NK5yWeNBjZXBlLgyx8b4TJX6U1Y//MlqNa3vokj7XoNN+hzjx+itrlLuG6/hml09MR56heJIkpscD0noAQMzrWBPcOOHO3nNf3yXyE55eQImwK79uIe5nNw5QVDfM+8Gi7RWP/t1HZ+xnh2Vjjhp2EQmkzOgJQiqHmOoQq+L9LJSvDSAtZ/3WYkQBVXWN2/bRARllPzXMG4Fl01t1wEIyau983+ByLyyyJyZde1i8iy1vqKYTYuIv1X+6CFhcXrA6+42ZVS7xGRWa31oVfqe43Pf0IpdVApdXBlKf/KH7CwsLghuB41/h4Rea9S6l0iEhKRhIj8oYiklFK+1bf7gIhMXO3DWuuHROQhEZHNtwzqq/WxsLC48bie+uy/IiK/IiKilHpARH5Ja/1RpdQXReSDIvI3IvIxEfnKNQdZRaVekcnVDK7lZbapM9qwpzycQZUIwZY7cxI22Y7bmOTixacRgpsY5hDWdBT2fNSDM4CVWSb/i8RhWy2dZU1ECT43N4/ftoX5U8IdEW5ZmOWQ2B/9FNxJ+RTbyqPjsA1LRvZWew9bSJtTw5jH1Ai1GV4uSfbhc3UP98vOGMQIaXZ1zi+g3ljYD5vXF2KXlATx+ER3z1HTO374k478mU//V0d+8P4foH7VPFx01ToTSXYY173sR6ZiVze7nX7kZ3A2PH7mj6nNJPh8+gWcfZSqfNax/t04p1g4wWcknjjuy2DgVmqL1mF/p9oRxpsPs7s0MwFilYUKr3c5hL5LVTyPfSF+hkfG8JzpEO+R0uo8appLVpt4NUE1n5LmYd15adrwn30VY1lYWNxgfF9BNVrrx0Xk8VX5oojc8XL9LSwsXj9Y0wg6rT1SbTQjytramTSichHm/PquYWp78cKII2/fChXu8qUj1C+dMggTXLTlngwimhYMvrGlIrve+jaAs75jho8YclmQDIyNY05aczRWrQZ1v22Q3WsDm6DqzdU3UFs4CjV2aADrMzbmKt20DiZJcJlLD4fasQbtRqnnYoUXJONB9ta6LfuobdZI7MpkYIYku/hxaRhZgOu6mOs/Foc6+sP/+gFHHn2C1dt3v+ftjhwIcvRb5wAiDJ95HJz6UmWzRjUMrvUIK6sL83Cl+mJY+2CWCUd8RXx3tMj3s95rkEhk+ZkoLsJ8CRoBkZkCuymHDC762ZSr/FPJ4AqchhperLGpO+mHWRnOuiL5dDOis9K4dilxGxtvYdEisJvdwqJFsLZU0tWaLI831Y2BAVbFUjGcgl+avUxtaSPYaXbOqJ7qmn4yCfXo4uQxalNdtzhyT9ogNIgwEYKagwr3na9wlFKxCBWpYlQ3rbqquAZq+A39hf/yCR4jBtMjVGGOsVmjtFWbF6fDnQ2mFD5+4mlHbo/zd/cEkbiyeAZVbRdrnJiha0gmWRrj8UNG9VAVhdoaFY4em53FCXMizu+N6UUkOoVCILJ44D72oETbDVIRL0eWVQ1NdXgPdGQlHfxdEyOOvHUHE2BsSMGbUKkiuuyFaSav8EbxPIba+BQ8a5yeTxeZWCW5FQ/nwjQ8BkOuslyLhmkQK7Apkzeen5DxXeXkDPW7exuiNl98+hFqi3ia3gVPw5JXWFi0POxmt7BoEdjNbmHRIlhTm115RPyJpovjwiSTsm/YsNeRR8aZLDKwCEK+aAw2ZNdGJn/wGWWYkp2cOVc0spACK3BJJZJsr377y+BaP32Al6e9A240kwzRnfUmIdhauQDb/dUa7N5sgbP20mEQZp4+d9CRY8I2ZDKBLLKJqdPU5jHcS54AzgRSbezqzExi/nNLTKLY5of9uhI2stfC/G4It+G7qjN8dnDRcFPGFFyMWlz2ah5nB5EoX2cibZB5HIGLscfluupII5NwfuF5aitPYF7ah+eot4fLj6XDuNd1xW6tuQuYc2eMbfHMNMbv7IK7beyy69ypD8/m0gJHuZUM11tnBOdC58d4jLNj/82R37mPSUi//cJTzbnXrOvNwqLlYTe7hUWLYG0j6KQu1XpTRYq2sTrU14OpVLMcWdZvVDs99MLjjuzvYlPAX9/iyENtTI4xPYvElTkjvG7uJEdLzZyCir/rFk6WOHTYSBCJYv6lHCe7/MQv/pAj5zIc4bZ32xsc+exlTqBRdaiZ3d1QYVcyHP02t4BEjWScXZiT44jo2jAA82JyhU2G9hh4xz2VLmqLdCEi7eQTuDbvLaxmxyJQp9uGuCLtQHDYkZeMNSiO8zxiMVzn9Cy7tfz9uBcZQ92vFl+ifoPJ+xy5r58JMCoTxj30IZGntsKqekRhHpdmOUFp+2ao56U8J3fuugUR49888G1H3rqNE2YunoC51eNKbBotwoVZbGB94jF+/rweRHsePf4MtaVXvZEvQ0Fn3+wWFq0Cu9ktLFoEdrNbWLQI1tRm93r8koz3iojIcoltpoyRdZQaZNK8mWnYUO1tcCdVS2xvD3QiLLNRZTKFzv5eR56bgDvsT/7gH6lfVwHnBXv33EdtkTBs1IaGO0b5OYOqcxt+Q6dyfJ2Lebj6QinOoNJZuGCyRghrPsLuKn8ELsd1bcz5PllD37qRDbaS4/DQdPseR66McXaVLwy78c7b73HkYBf3mzqHNW5Ls91/8hjCldsHcXbQ0cEhsbOjcC/V/exS66vh/voE9zY/zcQNPd1Y7927+LxnxHD1jV2GS/S3/+PvUb/zhf/syCkvu3QLFZwX5Bpc7+BJI3Q5moKNPXLgDPXrHsb6LOT4bCKYwrVdmDPOoaqcBRitItsxPMjuXp+nd/X/X5Nrwb7ZLSxaBHazW1i0CNZcjU8EmuqGP8TqUG4GKqyvh1VfXwLqS7wGlScY5qwxCUEtXmHvidSNckSFJUQ9hV1ECJ/6d7/jyH/8x39Ebd09cOdduoyos8E9HJ12cQpceB0dTGJQrUFFXpgbp7ZGHmrsQHqXI6dSrD6XvXDRFSscMdXRC/dj2cj803F237X1YM6ZOY7Cq88YGWbDUGHdJB3BANyPo5PM1x6NG21nUFq7c2gb9TMjEds7+F4sTCGScuf6nY485WdX59kRuCJrATYTcmWowoMDMHmeffZJ6lfdDtV6uG0rtV1eQjRjo8Y8fN4kntuuODLuBneHqN/xUazx+CSbVJFBqP81o5zz0Dp238UVokK9Id66nW3DIiLiD7h4Ag3YN7uFRYvAbnYLixbB2kbQ1RpSmW+eMusAc7/lfVC/9AJHWZUrOH3t9EOlX6lyksnpEzh53b3rfmqLFaHGLk9Btfv3//anqN/xkzgNnV9mdStpJNqUjSqgb/kgEzJoP0yU4hKrvhcLsC+8rI3Krt1Q+Z888oIj79zIUWF1H1TkXJDX8dLRrztybwAnwEUfr2nOqJSbTLM5pDL4vpJRDuDwAV7v7Ua02pZNO6ltxaAK70+jn58P0iWWAqdgocrJNKoIE+LAMUSMbd15J/XTSZgyeo7Xu7MbUX8xD8yErdtYVT+ksQYrFY569PixTcbHeY77OmAaZI2cqskEJ1hVg/hcWxvzEpYVFiXVQNvDn/8C9XvzB38Oc8pz8lI41EywqjduDJW0hYXFPyPYzW5h0SKwm93CokWwpja7R5SEPU3XwFiAyy5t7gF5xdwyu960D7Z4rgCbpLHEtk/PRtjzzx35BrXd3/4eR37Hnp9w5HqWM7l+/zOfwXx9/Fs4Mwtbrr0dpIe1AJct8vhgKyvh61xZwNnB9l62Gx95EnPeuBt27lMHX6B+CcM1tGPzemrLFVGqyBNGJlp3G7u1luZh63cEuDT1ZOUpRw5V4O4J+zhTMaDh5jl7+gK1RZNwkRYrOC+IZNju95Qwr4kxjnq8535k5qX68KhGUq5yy3nYw74Bvp/nD2E9Ovrh4upIccSfdxauspfOcw3TXVvwbHYN8EFLIAZbf3kCa1Arsm0/OooMx52DD1BbMQ03aNEgr+jfzNfi02g7c/EgtSlpPleVKhOL0uev2WIOpNSIiGRFpC4iNa31fqVUWkT+VkSGRWRERD6ktV661hgWFhY3F9+PGv8mrfVerfWVyoSfFpHHtNabReSx1X9bWFi8TvFq1Pj3icgDq/LnpVkD7lMv94FyrSQjqxVPZ4OssrX5oRR4FKtY4xfACze0EarNrr17qd+Y4V7rjrL7pDOIpJYOBdVxpsFqWTyJpJZolNXb2Tm0veG+YUfOz3I04K474Ya66FJNOzRMjbkSR9CtN0gSGoYaODDM5ZkS3VA5IxVW9cJpRPnlFuD+qXuY872WN8yhGEfXRRJwh50/AaKIzrZbqF9+EWqlr5OTgbJltI2OIzptW5xJLoZ7scYXJpjrv1SHGy0vSGLJTJ+jfpkK1jFfYdebJ4S1yuVxLw6+wAQYlRjWJ93JxCf5OdzfhcYIteWO47szFVxnr4fLIGZGYW6FNrmqs07hc+u6EQG5aT0nOc3NwxV82543Utv0dNM8ajRcdc8MXO+bXYvIt5RSh5RSV6oedGutr+yoaRHpvvpHLSwsXg+43jf7vVrrCaVUl4g8qpSiYGqttVZK6at9cPXH4RMiIsn2yNW6WFhYrAGu682utZ5Y/f+siPy9NEs1zyilekVEVv8/e43PPqS13q+13h+Nh67WxcLCYg3wim92pVRURDxa6+yq/DYR+U0R+aqIfExEPrP6/6+80lgNqUp29TchucIhoG0DcNWcm+bE/83rYcfUqlAgFoVrYUVqGHND/AEevwOc7J5O2OlHj3+X+m3bBDvp4Evs3mhU8du45y3gHVdRdkmdPwfuctXgLCRfGDZlOcR2tDeLcbo6ca7Q2cfjH7+ILLKEN01tdT+sqUgS3xXqYTsx1cD4x8aepralM3AX3nEXQlO9MeZaf+KbQY74SgAAE4xJREFU33TkjSWex8Y7cJ7SnsL98xXYtlfG+2bHJi77nKvC/mzzICNwbuo49fP0YsxAlR/pQBrnFgtTsK+X8hwivOk+nCV4OjjkNO7DmNEKW6vLYwh/ni5g/MgQu8DetwulqY9c/g61hRJGuegyzgviidup38Fx1AmcGWdii2C4+Zwpde339/Wo8d0i8vdKqSv9/0pr/U2l1Asi8gWl1MdF5LKIfOg6xrKwsLhJeMXNrrW+KCJ7rvL3BRF5842YlIWFxWuPNY2gC3pjsineVAtny8z5rkIw+f0RVkUyRonbi+dHHDnZxqV7a4tQbSbmWMV/8tBvOXI8jsuOLbE6dOeDUJ0On+RIqp//xX/nyJenv+TI61xlglUFGXwNP5MpBD1GZNkSR9dF23CmceYM1qfdlfWWCBt86lPMY55uRzRWVUG1njjMGXwbhrB2jQq7B9ffCr6+ZC+u7fJZzrRKRmGG9Kb4fXDyG+D2a7RDpe0L3kv96hHM8fnn2Hx7+wff6ciZAq5zocrmT/I8Iup6e7jsVzBs9O3Dug2u4/kmhzD+1577T9S23gfCjbkg8wHGFNZ7cAjfXc5zluGREwcced2+LdR2egnkG/Pe847ckWBOvkwd2XHdis2mzatzDMm35VqwsfEWFi0Cu9ktLFoEdrNbWLQI1tRmr9WrMpdt2uaLXs4Um54ziBOD/Bu0aBBEbt4JW6ua4dDIvRvBcd7fyyGsz5153JFnDDaavfvYhryYhR2dr/IcE92wz7pycEMdP8GuoGAFy5pq4/DF3BLOEsoRzu47dhRunDtue9CRL2U5661QgL3ds57tulIeZxC5Itaxw8ukmOPTI47cndxIbWE/wmVHTmK+vjqX0k4bYc3zS1xeeE7g3oz4MN7lZQ5PDntg277jnXzeOzsGG3V22SBlXHbfF6zB+DKfgzSKcFsmK2DPyReYxHPxEggzOzrZTZnLw05Paf7usBfPQdU4w2gEOKZke2q3I5cDLpdxCOuYNLjnszmm9QnFcP6TDHLG5+xoc12rlVcfLmthYfHPHHazW1i0CNaWvMLvk0RvU53Mz7uIEkdAMjC4hdXKjRvg/hkfgao7vsKuoIEBRHudHT9KbYuXRhx53z6UCDrn4m7fcttbHXnreiaSrBiq6ZlTUN17djA3/NkRmAnrhCPoxsNQ73TJpeKv4Lc3bASa+TSPkYxDnZvNsdraF4VLpmcjVMxYkd1rcytQ9wNx5p4fuYR7oxpY72iMVdhnnoZ58dYHH6S2RDfcd8UA1FvlojX3GhkVXmHVd3gDzJVnvgOTbWMXZ9/N5DBfH3spZWZixJFDAajnF2fOU7+BLrhEZ85yDkciiii8ofZBassaJKQLo/gub4xTRc7Ow8xp38FRhFLENrxgEEmm/Qnq1tGOe1ssuMqnZZpRf7U6u5JN2De7hUWLwG52C4sWwZqq8Upr8ayqGckgJyIEUkakk+bED5/HPHGGejQcZHX/wPe+6sjJXiZJ6O7H58YziNbbso053MbHkKTwMz/3SWrLBBCFVgxg/gvnufTR+RMnHHmpn1XTrZtQ1qnNRQHQezuScE6cB1ebJ8S674beYUcOeHLUdtsueCSeOQs1e3SOyTxqdaiSHY04tbUN4hS4UoYpcO4iq75v/sEfdOSlBY7kG+jBtYxNYL0zUzxGacNdkBvM29aeArHFtr241yef5tpe0SSej0DWlQiTh0oeXGdETgZYze40TvTf0PZ2ahuZwLUtxdibkJvHv3euv9uR58a4EvHmdTg9z2lOEB0wTJ6XTsHjsWEnk2j0RGBCnH7xMLXFr0Rf+q6aaS4i9s1uYdEysJvdwqJFYDe7hUWLYG1rvWkt9VLTLksE2b1Rrhm1sJLMcT42ChsnmUQkWFucM5DmFhGNlS+wbVUpw4YK9sKePH2Wo9/yC3Bp3PFWtrsCxtft3gByhpkVzuB7YNd+R9ZtvMQvngXRYVuZySD+7fs/7MiPPg9X5NIUR1KNnASphqeNXS0zBu9Cdw/WtOhyeS0vwhWXmWG33IYunCX42uGWUzmOWOwegk05Nu/KiCthXp3DyEqLR3qpnyeACR8+/wi19e+C6+3EaZyL3HvvD1G/Z773hCOn+jnL0J/C++zFSzjDeONuJjV99jncl/23v43aUgrRmPUARywqo/5BYQbnIMk49xvP4Kyi4LKruxpYnzv64GY+OcZZl5uiiPbs6uKst0Ju1db38D0yYd/sFhYtArvZLSxaBGuqxldrZRlfGBERkZExdlcNboDq3rjEJYLSHXCPnTwCfretg5wMECnDzeVL8KWlEnBb1GvoV/JwNFM9AvVzcYGTO2YVyvts7zTKInWyuj9xCupig6co6/pudWSvK5zsq4980ZGDPqjg0SAnsXQPwlW2UB6lNl8BUXnjo3DLreRGqF8wj+seSA9T21wWJlBFYNbs3cFc6PN5JJN0pdnVuZyBaRCLY75ev4uYJARX0+ZNG6jt+PMoh5WOwWV5/CyrtxKAqdHRzVz/Ko61GzJcdIVpToTxJ+DOO3yMOfl6hmFORDWXnipqfC6Qw7V5u5jHrpHEM+etc/RbNofnJxqFrRgVJmepB6CiVy5w5GTy7ub+8YZc0XkG7JvdwqJFYDe7hUWLwG52C4sWwdq63pSWur9pk+zayoR/FR9s5RR7FaSzG3/oCMMILuXZLur3wp03X+CMskuGzXprAi6M6Rr/3m0fgG2Y6WWSxoxBpjBXhP3nooaXYBhjjizyGHfcDbfOpQtcsyzmw7VdOgpbvK2NOchzYWRDreR5/pHNxvmBUXJ6cJrt4YoRLrrU4LOJzg5k8VXCRm26JZ5vbhnjB0PsBo12IOQ5bJyD1ErsGrp9H56DySN8P+sxXEtvCPbrTIWJKcMGmUe5xOcnuSzOPiqLuH9b7uA6gRcvwfaOlTh8WBtnAnlXBmI0CfLIvjbcvxNLz3C/ONZxeZLvWUccNnzWi+9Kp/l8oHcA91DX+Z5lcs0zsEaD74MJ+2a3sGgR2M1uYdEiWFM13hfwSee6ZuSSd4ojnaIdIEbo6uDIslgC6nlPDOwE+QKrMs+Nw+XVcR+TDNRmMcZ4BZ/z15mrLt0FNf6FwweobbAP5AeBICKwJkocPVaYg0q4b9/91NZhZHJd1KyOev1wld1xH7K8zi/wPGaCUOsLGY7UkjyuZ34B47dXdlK3Rsggokiw3TR9Gep0JXUKYyRYhe1vQzTckQMcibi9y7xPmFPIx27Es5dRfivhY1KKs3NPOvI9t33UkR/7U640dufbwOuedkVf1n24lpKAqCSW5mzB4ALWMVzmbdHVgPp8YYHdfvF2mBDxBEwXH1tekjdMzvIyR482DDV+eDOiO0eP8pomtsBMGItyBGBuuXmv6/VXGUGnlEoppf5OKXVaKXVKKXW3UiqtlHpUKXVu9f9trzyShYXFzcL1qvF/KCLf1Fpvk2YpqFMi8mkReUxrvVlEHlv9t4WFxesU11PFNSkibxSRHxMR0VpXRKSilHqfiDyw2u3zIvK4iHzq5cbSWqRSaZ7MDu9h0ohnn3nWkbe56rhnJnEyXS2CD0z7OEFk4x6cjF46zVF47QlUCC2sgMjBHdF1YAEReokin8pKHarY2AoqalYqnNgwPwayg91v5hC6sWmoxckYUxZ3etC3nkZST7uXo8JeeAlUxOlB5oXLLEDV3joEIotsg9cqO21E6OXYpKpFsf7emKGPeviku9MoNbXjfi6BtXmdUQ33BEoSFX1McpEr47vCXVwWaeUc5rwyjWfg/R/5CPU7PQkzKtrN9yxRxhwzPlxnZpwj6EIekKd0bGfSiFIOUX6D65hw5Ow4IgXLy+DCK+SZ169nE07WO2KsBFcr8FacegmkFNu691G/Yye/58ge19YdXE2gCfiZe9HE9bzZ14vInIj8D6XUYaXUf18t3dyttb6ya6ZFXLQrFhYWrytcz2b3ichtIvKnWutbRSQvLpVda63F5IsyoJT6hFLqoFLqYH6ldLUuFhYWa4Dr2ezjIjKutb5yJPx30tz8M0qpXhGR1f/PXu3DWuuHtNb7tdb7o8nQ1bpYWFisAa6nPvu0UmpMKbVVa31GmjXZT67+9zER+czq/7/yMsOIiIjHE5BYvOlWGxnnbK1uP1wJsz6OpCrn4TJJb4NLZ3yG7dV9++Fe8jzH9nCyHRF1KysYIxZlm1pVEIH0pve+k9rOnQFJwrkCbKtNm9le3ZhChF6ozgpPmxGdNll/ntqWp/B7WYnDNRSNMTnnm28BP/6xwmPUtnUjrKm64ebyutLvijM4t6iyyS6bN8OldjaDefhC/G44tgxXZzLGhO0HvvOUI7f1wz2VGuJ5jFxChOFKid2gd70BvP39Mdi851ylpnYOoLTSyASPEQ/B7l/M4Jmr1Plaal6cddQWOPptw164BEeOcr2DPVsxx+oi7t/py6epn4oikzAd5HOFDoMevpjHdZ68dIT6tW3A2U29wiWkvMvNz6n6tbf09frZf0ZE/lIpFRCRiyLy49LUCr6glPq4iFwWkQ9d51gWFhY3Ade12bXWL4nI/qs0vfkqf7OwsHgdYk0j6AIenwwEm2rmwWMnqO2edyFBZGKOI9I86xCZdPLbUB3DQXYAHHoErolCkqO9KlVEFu3YiiSIzBK76HbE4Hb58rf+mto2d0KdTrQjyi/iZ1dKweB5V0Eu4dMwzAZvgt0zoRWjrNM8oqrC7S4O/E347hdfYI79hTm4Js+9iAisO+59K/VTdSNiLM3RWLodrqDBKlTOI7N8X+5qw+9/0cumxp73whTIjEPlXHGZaCoA15tJYCIiMjUFXrjnTz2Hfns5OnL0qMFFP8Ru284+XNvcJNx+yT6uObA9tdWRz46yeVWs49r6+/jcaWUOLryA4YJNd7G7dKOhgqtRRW3lIsyLRYPP3x9hE3DyMkyqni5e785Ik0zF57HkFRYWLQ+72S0sWgR2s1tYtAjWlnCyUZXpfDMjbP+b3kRtpQqmoqsZalscgftkawfCXvNpTtRPZmDDV9LsmogZ5AEXjx9z5O37tlG/x55+1JE3bdlFbXUvstnaYviu5WqW+m1cjzFPHefMtngP7OGzlzg0ocsH+35oE8JNva6w3ZfOPOzIe9q4XHQ0gnW87wPvd2R/id2UO/bCfZdOc9bbswf+0ZHD3XgfbB1kAozDF0Aaes9tD1Db9CxcanoRY2Q08/lvGISraWKa+eszJdzf4V2w00dH2fW2dR/cX+emOVNsfgLft/8NqE136tBL1E/1wdYNeF1nMMaZQyPO9nbQi3uTNZ6D7bdyBl92CnZ5xigjLSKS6Mf5Rv8QiEwnjnNIb8cQbPhqg9PqMqp5JlW/emybiNg3u4VFy8BudguLFoFqhrWv0ZcpNSfNAJwOEZl/he43Gq+HOYjYebhh58H4fucxpLXuvFrDmm5250uVOqi1vlqQTkvNwc7DzmMt52HVeAuLFoHd7BYWLYKbtdkfuknfa+L1MAcROw837DwYr9k8borNbmFhsfawaryFRYtgTTe7UuodSqkzSqnzSqk1Y6NVSv25UmpWKXXc+NuaU2ErpQaVUt9VSp1USp1QSv3czZiLUiqklHpeKXVkdR6/sfr39UqpA6v3529X+QtuOJRS3lV+w4dv1jyUUiNKqWNKqZeUUgdX/3YznpEbRtu+ZptdKeUVkf8qIu8UkR0i8hGl1I6X/9Rrhs+JyDtcf7sZVNg1EflFrfUOEblLRD65ugZrPZeyiDyotd4jIntF5B1KqbtE5HdE5Pe11ptEZElEPn6D53EFPydNevIruFnzeJPWeq/h6roZz8iNo23XWq/JfyJyt4g8Yvz7V0TkV9bw+4dF5Ljx7zMi0rsq94rImbWaizGHr4jIW2/mXEQkIiIvisid0gze8F3tft3A7x9YfYAfFJGHRUTdpHmMiEiH629rel9EJCkil2T1LO21nsdaqvH9ImKWNB1f/dvNwk2lwlZKDYvIrSJy4GbMZVV1fkmaRKGPisgFEVnWWl8h61ur+/MHIvLLInIlQ6j9Js1Di8i3lFKHlFKfWP3bWt+XG0rbbg/o5OWpsG8ElFIxEfmSiPy81ppS/NZqLlrrutZ6rzTfrHeIyLZX+MhrDqXUe0RkVmt96BU733jcq7W+TZpm5ieVUm80G9fovrwq2vZXwlpu9gkRMfmEBlb/drNwXVTYrzWUUn5pbvS/1Fp/+WbORUREa70sIt+VprqcUkpdyZFdi/tzj4i8Vyk1IiJ/I01V/g9vwjxEaz2x+v9ZEfl7af4ArvV9eVW07a+EtdzsL4jI5tWT1oCIfFhEvrqG3+/GV6VJgS1ynVTYrxZKKSUinxWRU1rr37tZc1FKdSqlUqtyWJrnBqekuek/uFbz0Fr/itZ6QGs9LM3n4Tta64+u9TyUUlGlVPyKLCJvE5Hjssb3RWs9LSJjSqkrhHhXaNtfm3nc6IMP10HDu0TkrDTtw/9zDb/3r0VkSkSq0vz1/Lg0bcPHROSciHxbRNJrMI97pamCHRWRl1b/e9daz0VEdovI4dV5HBeRX139+wYReV5EzovIF0UkuIb36AERefhmzGP1+46s/nfiyrN5k56RvSJycPXe/IOItL1W87ARdBYWLQJ7QGdh0SKwm93CokVgN7uFRYvAbnYLixaB3ewWFi0Cu9ktLFoEdrNbWLQI7Ga3sGgR/P9yw5LSpCwGLgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Class of this picture is: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx99u_dYm-0C"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "x_train /= 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RUtc_gr6Iy2",
        "outputId": "98cdcd21-a013-4156-a9cb-8c7d0f322c4b"
      },
      "source": [
        "# Your code goes here\n",
        "\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(64, kernel_size=3, activation=\"relu\",input_shape=(64,64,3)))\n",
        "  model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(3, activation=\"softmax\"))\n",
        "\n",
        "  return model\n",
        "\n",
        "model = create_model()\n",
        "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, validation_split=0.3, epochs=8, batch_size=64)\n",
        "\n",
        "print(y_train.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "27/27 [==============================] - 29s 1s/step - loss: 1.3691 - accuracy: 0.4312 - val_loss: 0.9614 - val_accuracy: 0.4944\n",
            "Epoch 2/8\n",
            "27/27 [==============================] - 28s 1s/step - loss: 0.8220 - accuracy: 0.5644 - val_loss: 0.7175 - val_accuracy: 0.6319\n",
            "Epoch 3/8\n",
            "27/27 [==============================] - 28s 1s/step - loss: 0.6248 - accuracy: 0.7245 - val_loss: 0.6999 - val_accuracy: 0.6889\n",
            "Epoch 4/8\n",
            "27/27 [==============================] - 28s 1s/step - loss: 0.5097 - accuracy: 0.7925 - val_loss: 0.7563 - val_accuracy: 0.6431\n",
            "Epoch 5/8\n",
            "27/27 [==============================] - 28s 1s/step - loss: 0.3702 - accuracy: 0.8520 - val_loss: 0.7923 - val_accuracy: 0.6486\n",
            "Epoch 6/8\n",
            "27/27 [==============================] - 28s 1s/step - loss: 0.2368 - accuracy: 0.9139 - val_loss: 0.8274 - val_accuracy: 0.6778\n",
            "Epoch 7/8\n",
            "27/27 [==============================] - 28s 1s/step - loss: 0.1382 - accuracy: 0.9679 - val_loss: 0.9392 - val_accuracy: 0.6569\n",
            "Epoch 8/8\n",
            "27/27 [==============================] - 28s 1s/step - loss: 0.0850 - accuracy: 0.9772 - val_loss: 1.0878 - val_accuracy: 0.6472\n",
            "(2400, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxLQ5V38o15J",
        "outputId": "a2258233-814a-43c9-99a2-bf7a82597721"
      },
      "source": [
        "# Then, you will use the trained model to get y_pred, and this will be sent to us\n",
        "\n",
        "y_pred = model.predict(x_test) # make prediction here\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "print(y_pred)\n",
        "# Please use the following code to evaluate your result,  this is a necessary step, best regards!\n",
        "# Make sure the input of the evaluation is 1D array!\n",
        "task.evaluate(y_pred.flatten().tolist(), model)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 0 1 0 0 0 2 1 0 0 0 2 0 0 0 2 0 0 0 0 0 0 1 0 2 2 1 2 1 1 2 0 2 1 1 1\n",
            " 1 1 0 1 1 0 2 2 0 0 0 1 0 1 1 2 2 1 2 0 2 1 0 2 1 1 2 0 2 1 1 2 2 2 1 1 1\n",
            " 0 1 0 1 1 0 0 0 0 0 1 1 2 2 1 2 1 0 2 0 0 2 1 0 0 1 0 1 0 1 2 0 0 1 1 2 1\n",
            " 1 2 1 1 2 1 2 2 1 2 0 2 0 1 2 1 0 0 2 2 0 1 2 1 2 2 0 2 2 1 0 2 0 1 0 0 1\n",
            " 2 0 2 2 1 0 0 1 1 1 0 1 2 2 2 0 2 2 2 0 2 1 0 1 2 2 1 0 1 0 1 0 0 2 2 1 1\n",
            " 2 1 2 2 0 2 2 2 1 2 0 2 1 0 2 2 1 2 0 1 0 1 1 0 0 0 2 1 0 2 0 2 0 0 0 0 2\n",
            " 2 2 0 1 2 0 2 2 1 2 1 1 2 2 0 0 0 2 2 2 0 0 2 1 0 0 2 1 0 2 1 1 1 2 1 2 1\n",
            " 0 0 2 0 2 0 2 0 2 2 1 0 0 0 2 1 0 0 0 0 1 0 2 0 2 0 2 1 0 2 0 2 2 2 0 1 0\n",
            " 1 2 2 0 0 2 0 2 0 1 2 1 2 2 1 0 2 1 0 1 0 0 2 0 2 2 1 0 0 2 2 2 1 0 0 0 1\n",
            " 2 0 0 0 2 0 1 0 0 0 2 0 0 1 2 1 1 1 0 0 1 2 0 1 0 0 0 0 0 1 2 1 0 0 0 0 0\n",
            " 2 0 2 0 2 2 2 2 0 0 2 0 0 1 0 2 1 2 1 1 2 0 2 2 1 1 0 0 2 1 1 0 1 0 0 0 0\n",
            " 0 1 0 0 1 2 0 2 2 2 2 0 0 0 2 0 2 0 0 1 1 1 1 0 0 0 0 1 1 2 0 1 1 1 2 2 0\n",
            " 0 2 2 2 1 1 1 1 0 1 0 2 1 1 0 1 2 2 0 1 1 1 0 2 2 0 0 2 0 2 1 0 1 1 0 1 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 2 1 1 0 1 2 2 0 2 0 2 0 1 1 2 0 2 1 2 1 1 0\n",
            " 2 0 1 0 1 2 0 0 0 0 2 0 1 2 2 2 1 1 2 0 2 1 1 2 1 0 1 1 0 0 0 0 2 2 2 2 2\n",
            " 1 2 1 2 1 0 2 0 0 0 0 0 2 2 1 0 0 1 0 2 2 1 0 0 1 2 2 0 2 0 1 2 2 0 0 2 0\n",
            " 2 2 1 2 1 0 2 2]\n",
            "The accuracy of your result is: 0.6466666666666666\n",
            "Thank you URN: 6569605, we've received your result!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}